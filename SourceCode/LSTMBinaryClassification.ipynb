{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0C8nV6tqatm"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title here link your goole drive directory that contains the files\n",
        "MOUNTPOINT = '/content/gdrive'\n",
        "directoryInsideGoogleDrive = 'HERE YOU DIR NAME'\n",
        "DATADIR = os.path.join(MOUNTPOINT, 'MyDrive', f{directoryInsideGoogleDrive})\n",
        "drive.mount(MOUNTPOINT,force_remount=True)"
      ],
      "metadata": {
        "id": "5sPtZvAyqcJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LSTM class\n",
        "class LSTM_NN(torch.nn.Module):\n",
        "  def __init__(self, input_features_num:int, LSTM_hidden_layers_num:int, Dropout:float):\n",
        "    super(LSTM_NN,self).__init__()\n",
        "    self._LSTM = torch.nn.LSTM(input_features_num,LSTM_hidden_layers_num,batch_first=True,num_layers=1)\n",
        "    for names, params in self._LSTM.named_parameters():\n",
        "      if('weight_in' in names or 'weight_hh' in names):\n",
        "        torch.nn.init.xavier_uniform_(params.data)\n",
        "      elif('bias' in names):\n",
        "        params.data.fill_(0)\n",
        "    #layer norm od lstm\n",
        "    self.layer_norm = torch.nn.LayerNorm(LSTM_hidden_layers_num)\n",
        "    #droput\n",
        "    self.dropout = torch.nn.Dropout(Dropout)\n",
        "\n",
        "    # 2 fcc layers inner layers\n",
        "    self.FCC_1 = torch.nn.Linear(LSTM_hidden_layers_num, LSTM_hidden_layers_num)\n",
        "    self.layer_norm_fcc1 = torch.nn.LayerNorm(LSTM_hidden_layers_num)\n",
        "\n",
        "    self.FCC_2 = torch.nn.Linear(LSTM_hidden_layers_num, int(LSTM_hidden_layers_num/2))\n",
        "    self.layer_norm_fcc2 = torch.nn.LayerNorm(int(LSTM_hidden_layers_num/2))\n",
        "\n",
        "    #output layer\n",
        "    self.FCC_3 = torch.nn.Linear(int(LSTM_hidden_layers_num/2),1) #binary classsification\n",
        "\n",
        "  def forward(self, input_data_tensor:torch.tensor):\n",
        "      #lstm output\n",
        "      LSTM_output, _ = self._LSTM(input_data_tensor)\n",
        "      #lstm output layer norm\n",
        "      LSTM_output = self.layer_norm(LSTM_output)\n",
        "      #lstm output dropout\n",
        "      LSTM_output = self.dropout(LSTM_output)\n",
        "\n",
        "      #fcc1\n",
        "      output = self.FCC_1(LSTM_output[:,-1,:])\n",
        "      output = self.layer_norm_fcc1(output)\n",
        "      output = torch.tanh(output)\n",
        "      output = self.dropout(output)\n",
        "\n",
        "      #fcc2\n",
        "      output = self.FCC_2(output)\n",
        "      output = self.layer_norm_fcc2(output)\n",
        "      output = torch.tanh(output)\n",
        "      output = self.dropout(output)\n",
        "\n",
        "      #fcc3\n",
        "      output = self.FCC_3(output)\n",
        "      output = torch.sigmoid(output).view(-1,1,1)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "jxbik3LSqt7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title shuffle data, needs same seed for train, validation and test, default seed is enough\n",
        "def shufflePathsAndSplitForTrainValidationtest(dirNameWithFiles:str,seed:int = 234):\n",
        "    #returns a list of random paths to the files for training, validation and test\n",
        "    #vraca listu random pathova za train, validate i test\n",
        "    random.seed(seed)\n",
        "    listOfPaths = os.listdir(os.path.join(DATADIR,f\"{dirNameWithFiles}\"))\n",
        "    listOfPaths = [os.path.join(DATADIR,f\"{dirNameWithFiles}/{fileName}\") for fileName in listOfPaths]\n",
        "    random.shuffle(listOfPaths)\n",
        "    #split  80 , 10 ,10\n",
        "    totalNumberOfFiles = len(listOfPaths)\n",
        "    trainListOfPaths = listOfPaths[:int(totalNumberOfFiles*0.8)]\n",
        "    validateListOfPaths = listOfPaths[int(totalNumberOfFiles*0.8):int(totalNumberOfFiles*0.9)]\n",
        "    testListOfPaths = listOfPaths[int(totalNumberOfFiles*0.9):]\n",
        "    #return\n",
        "    return trainListOfPaths, validateListOfPaths, testListOfPaths"
      ],
      "metadata": {
        "id": "_EhtPWfYq8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title mean i std used for data normalization\n",
        "def GetMeanAndStdOdTrainData(TrainListOfPaths:list[str,str], NumberOfFeatures:int = 3):\n",
        "  #returns mean and std of train data\n",
        "  #Welford's method\n",
        "  n = 0\n",
        "  mean = np.zeros(NumberOfFeatures)\n",
        "  M2 = np.zeros(NumberOfFeatures)\n",
        "  for path in TrainListOfPaths:\n",
        "      trainData = pd.read_csv(path,header=None)\n",
        "      trainData = trainData.to_numpy()\n",
        "      #all measurements into one row\n",
        "      trainData = (trainData[:,:NumberOfFeatures])\n",
        "      totalNumberOfMeasurements = trainData.shape[0] #sequence number\n",
        "      for i in range(totalNumberOfMeasurements):\n",
        "        n += 1\n",
        "        delta = trainData[i,:] - mean\n",
        "        mean += delta / n\n",
        "        delta2 = trainData[i,:] - mean\n",
        "        M2 += (delta * delta2)\n",
        "\n",
        "  variance = M2 / (n - 1)\n",
        "  std = np.sqrt(variance)\n",
        "\n",
        "  return mean, std"
      ],
      "metadata": {
        "id": "Vu0CLUlzsKoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom DataLoader\n",
        "def CustomDataLoader(CurrentBatchFilePaths:list[str,str],cpuTest = False,FeaturesNumber:int=3):\n",
        "    #loads a list that contains a current batch paths to the csv files and then creates tensors\n",
        "    #for them ordered to keep the label and train pairs ordering\n",
        "    listOfInputsMeasurements = []\n",
        "    listOfLabelsMeasurements = []\n",
        "    for csvMeasurementPath in CurrentBatchFilePaths:\n",
        "        TempDataframe = pd.read_csv(csvMeasurementPath,header=None)\n",
        "        #fill the list with third dimencion for later stacking\n",
        "        TempNumpyArray = TempDataframe.to_numpy()\n",
        "        TempNumpyArray = TempNumpyArray[None,:,:]\n",
        "        listOfInputsMeasurements.append(TempNumpyArray[:,:,:FeaturesNumber]) #all rows, columns 1,2,3\n",
        "        listOfLabelsMeasurements.append(TempNumpyArray[:,0,3].reshape(1,1,1)) #first row and column 4\n",
        "    #stack\n",
        "    InputData = np.concatenate(listOfInputsMeasurements,axis=0)\n",
        "    LabelsData = np.concatenate(listOfLabelsMeasurements,axis=0)\n",
        "    #witch to Torch\n",
        "    InputData = torch.tensor(InputData, dtype = torch.float32)\n",
        "    LabelsData = torch.tensor(LabelsData, dtype = torch.float32)\n",
        "    #device select, force cuda\n",
        "    if(torch.cuda.is_available() == True):\n",
        "        InputData.to('cuda')\n",
        "        LabelsData.to('cuda')\n",
        "    elif(torch.cuda.is_available() == False and cpuTest==False):\n",
        "        raise Exception(\"Cuda not avaliable!!\")\n",
        "    #return input first and then labels\n",
        "    return InputData, LabelsData"
      ],
      "metadata": {
        "id": "Hd7b-7G9s-MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title heleper function for later saving the model and loss results\n",
        "def SaveTrainValidationLoss(train_losses, validation_losses):\n",
        "    'train'\n",
        "    train_losses_dataframe = pd.DataFrame(train_losses,columns=['losses'])\n",
        "    CsvPathForTrainLosses=os.path.join(DATADIR,f'Model/Train_Losses.csv')\n",
        "    #check if exist alerady to append to or make new\n",
        "    if not pd.io.common.file_exists(CsvPathForTrainLosses):\n",
        "        train_losses_dataframe.to_csv(CsvPathForTrainLosses, index=False)\n",
        "    else:\n",
        "        train_losses_dataframe.to_csv(CsvPathForTrainLosses,mode='a',header=False,index=False)\n",
        "\n",
        "    'validation'\n",
        "    validation_losses_dataframe = pd.DataFrame(validation_losses,columns=['validation'])\n",
        "    CsvPathForValidationLosses=os.path.join(DATADIR,'Model/Validation_Losses.csv')\n",
        "    if not pd.io.common.file_exists(CsvPathForValidationLosses):\n",
        "        validation_losses_dataframe.to_csv(CsvPathForValidationLosses, index=False)\n",
        "    else:\n",
        "        validation_losses_dataframe.to_csv(CsvPathForValidationLosses,mode='a',header=False,index=False)"
      ],
      "metadata": {
        "id": "hBWkx0smuJfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main Training function\n",
        "def TrainNetwork(FeaturesNumber:int, LSTM_hidden_num:int, Dropout_Value:int,\n",
        "                 epoch_num:int, batch_size:int,\n",
        "                 LossFunction = torch.nn.BCELoss, Optimizer = torch.optim.Adam, l2=1e-6,\n",
        "                 learning_rate:float = 0.1,\n",
        "                 exponential_learning_rate_scheduler_gamma = 0.99,\n",
        "                 max_grad_clip_value = 10.0,\n",
        "                 validation_patience : int = 100,\n",
        "                 precalculated_mean_std:bool = False,\n",
        "                 precalculated_mean:torch.tensor = None ,\n",
        "                 precalculated_std:torch.tensor = None,\n",
        "                 continueToTrain : bool = False,\n",
        "                 dirNameWithFiles:str=None):\n",
        "    #using classification LOSS Binary Cross Entropy\n",
        "    #can use precalculated mena and std to speed up parts\n",
        "    if (torch.cuda.is_available() == True):\n",
        "        device= \"cuda\"\n",
        "    else:\n",
        "        raise Exception('Cuda not properly setup!!')\n",
        "    #check for directory for model and metadata and if notexistanta create it\n",
        "    if(os.path.isdir(\"content/Model\") == False):\n",
        "        os.mkdir(os.path.join(DATADIR,'Model'))\n",
        "\n",
        "    #load existing model or make new\n",
        "    if(continueToTrain == True):\n",
        "        print('Continuing to train')\n",
        "        modelPath = os.path.join(DATADIR,'Model/ModelWithMetadata.pth')\n",
        "        Checkpoint = torch.load(modelPath)\n",
        "        model = LSTM_NN(input_features_num = Checkpoint['input_features_num'],\n",
        "                        LSTM_hidden_layers_num= Checkpoint['LSTM_hidden_layers_num'],\n",
        "                        Dropout = Checkpoint['dropout'])\n",
        "        model.load_state_dict(Checkpoint    ['model_state_dict'])\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        'CREATING LSTM Model'\n",
        "        model = LSTM_NN(input_features_num = FeaturesNumber,\n",
        "                        LSTM_hidden_layers_num = LSTM_hidden_num,\n",
        "                        Dropout = Dropout_Value)\n",
        "        model = model.to(device)\n",
        "    #loss\n",
        "    selected_loss = LossFunction().to(device)\n",
        "    #randomize and split data\n",
        "    train_data_CSV_path, validate_data_CSV_path, test_data_CSV_path =\\\n",
        "                             shufflePathsAndSplitForTrainValidationtest(dirNameWithFiles)\n",
        "    #mean and std for normalization\n",
        "    if(precalculated_mean_std == False):\n",
        "      print('creating --> 80,10,10 <-- split and calcuating Mean and Std of train data')\n",
        "      mean,std = GetMeanAndStdOdTrainData(train_data_CSV_path, FeaturesNumber)\n",
        "      mean,std = torch.tensor(mean, dtype = torch.float32), torch.tensor(std, dtype = torch.float32)\n",
        "      mean,std = mean.to(device), std.to(device)\n",
        "      print('mean')\n",
        "      print(mean)\n",
        "      print('std')\n",
        "      print(std)\n",
        "      print('done')\n",
        "    else:\n",
        "      mean,std = precalculated_mean.to(device), precalculated_std.to(device)\n",
        "    print()\n",
        "    print('Training:')\n",
        "    TrainingLosses = []\n",
        "    ValidationLosses = []\n",
        "    previousValidationLoss = float('inf')\n",
        "    improvementsTracker = 0\n",
        "    selected_optimizer = Optimizer(model.parameters(),lr=learning_rate, weight_decay=l2)\n",
        "    #or use decay that is commented\n",
        "    #selected_learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(selected_optimizer,\n",
        "    #                                              gamma= exponential_learning_rate_scheduler_gamma)\n",
        "    for epoch in range(epoch_num):\n",
        "        print(f'==== EPOCH: {epoch} ====')\n",
        "        ############ train mode #############\n",
        "        'switching into training mode'\n",
        "        model.train()\n",
        "        totalTrainLoss = 0\n",
        "        num_batches = 0\n",
        "        random.shuffle(train_data_CSV_path)\n",
        "        random.shuffle(validate_data_CSV_path)\n",
        "        #iterating over a list of paths and evokuing custom dataloader\n",
        "        for batch_index in range(0,len(train_data_CSV_path),batch_size):\n",
        "            'loading training batch and creating formated tensor data and labels'\n",
        "            torch.cuda.empty_cache()\n",
        "            data,labels= \\\n",
        "                CustomDataLoader(train_data_CSV_path[batch_index:batch_index+velicina_batcha],FeaturesNumber=FeaturesNumber)\n",
        "            data,labels = data.to(device), labels.to(device)\n",
        "            'normalize data'\n",
        "            data = (data - mean)/std\n",
        "            'forward propagation'\n",
        "            model.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = selected_loss(output,labels)\n",
        "            totalTrainLoss += loss.item()\n",
        "            'backward propagation'\n",
        "            loss.backward()\n",
        "            'clip gradient'\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_clip_value)\n",
        "            'weight update'\n",
        "            selected_optimizer.step()\n",
        "            print(f'\\r---> LOSS: {loss.item()} ',end='')\n",
        "            #increase number of batches tracker\n",
        "            num_batcheva += 1\n",
        "        #average value through the epoch\n",
        "        AverageTrainingLoss = totalTrainLoss/broj_batcheva\n",
        "        TrainingLosses.append(AverageTrainingLoss)\n",
        "        #if decide to use scheduler was selected uncomment next line\n",
        "        #selected_learning_rate_scheduler.step()\n",
        "\n",
        "\n",
        "        ########## evaluation mode ############\n",
        "        'switching into evaluation mode'\n",
        "        model.eval()\n",
        "        TotalValidationLoss = 0\n",
        "        num_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_index in range(0,len(validate_data_CSV_path),batch_size):\n",
        "                'loading validtion batch and creating formated tensor data and labels'\n",
        "                torch.cuda.empty_cache()\n",
        "                data,labels= \\\n",
        "                    DataLoaderKojiVracaTorchTensorInputaILabela(validate_data_CSV_path[batch_index:batch_index+batch_size])\n",
        "                data,labels = data.to(device), labels.to(device)\n",
        "                'normalize'\n",
        "                data = (data - mean)/std\n",
        "                output = model(data)\n",
        "                loss = selected_loss(output,labels)\n",
        "                TotalValidationLoss += loss.item()\n",
        "                ValidationLosses.append(TotalValidationLoss)\n",
        "                num_batches += 1\n",
        "            #epoch average\n",
        "            print()\n",
        "            AverageValidationLoss = TotalValidationLoss/num_batches\n",
        "            ValidationLosses.append(AverageValidationLoss)\n",
        "            print(f'==== Results of epoch {epoch} :Average Training LOSS --> {AverageTrainingLoss:.12f},Average Validation LOSS --> {AverageValidationLoss:.12f} ====')\n",
        "            'early stop check'\n",
        "            if(np.round(AverageValidationLoss,4) >= np.round(previousValidationLoss,4)):\n",
        "                improvementsTracker += 1\n",
        "                if(improvementsTracker >= validation_patience):\n",
        "                    print('Early Stop')\n",
        "                    break\n",
        "            else:\n",
        "                previousValidationLoss = AverageValidationLoss\n",
        "\n",
        "    'Metadata saving'\n",
        "    print()\n",
        "    print('Saving Model, Model Metadata, train and validation loss')\n",
        "    SaveTrainValidationLoss(TrainingLosses, ValidationLosses)\n",
        "    torch.save({\n",
        "            'input_features_num': FeaturesNumber,\n",
        "            'LSTM_hidden_layers_num': LSTM_hidden_num,\n",
        "            'dropout': Dropout_Value,\n",
        "            'model_state_dict': model.state_dict()\n",
        "            },os.path.join(DATADIR,'Model/ModelWithMetadata.pth'))\n",
        "    print('done')"
      ],
      "metadata": {
        "id": "UJeMpvMnu1QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test the network\n",
        "def testNN(FeaturesNumber,\n",
        "           batch_size:int,\n",
        "           separate_test_dir:bool,\n",
        "           precalculated_mean_std:bool = False,\n",
        "           precalculated_mean:torch.tensor = None ,\n",
        "           precalculated_std:torch.tensor = None):\n",
        "    if (torch.cuda.is_available() == True):\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        raise Exception('Cuda nije dobro postavljena')\n",
        "    PathDoModela = os.path.join(DATADIR,'Model/ModelWithMetadata.pth')\n",
        "    Checkpoint = torch.load(PathDoModela)\n",
        "    model = LSTM_NN(input_features_num = Checkpoint['input_features_num'],\n",
        "                    LSTM_hidden_layers_num= Checkpoint['LSTM_hidden_layers_num'],\n",
        "                    Dropout = Checkpoint['dropout'])\n",
        "    model.load_state_dict(Checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    if(zasebni_test_folder == True):\n",
        "      test_data_CSV_path = os.listdir(os.path.join(DATADIR,\"SeparateDir\"))\n",
        "      test_data_CSV_path = [os.path.join(DATADIR,f\"SeparateDir/{fileName}\") for fileName in test_data_CSV_path]\n",
        "    else:\n",
        "      train_data_CSV_path, _ , test_data_CSV_path = shufflePathsAndSplitForTrainValidationtest(dirNameWithFiles)\n",
        "    #determine mean std on train data\n",
        "    if(precalculated_mean_std == False):\n",
        "      print('creating --> 80,10,10 <-- split and calcuating Mean and Std of train data')\n",
        "      mean,std = GetMeanAndStdOdTrainData(train_data_CSV_path, FeaturesNumber)\n",
        "      mean,std = torch.tensor(mean, dtype = torch.float32), torch.tensor(std, dtype = torch.float32)\n",
        "      mean,std = mean.to(device), std.to(device)\n",
        "      print('mean')\n",
        "      print(mean)\n",
        "      print('std')\n",
        "      print(std)\n",
        "      print('done')\n",
        "    else:\n",
        "      mean,std = precalculated_mean.to(device), precalculated_std.to(device)\n",
        "    model.eval()\n",
        "    print()\n",
        "    print(\"TESTING: \")\n",
        "    Labels_List_For_Whole_Column = []\n",
        "    Output_List_For_Whole_Column = []\n",
        "    with torch.no_grad():\n",
        "        for batch_index in range(0,len(test_data_CSV_path),batch_size):\n",
        "            'loading test batch and creating formated tensor data and labels'\n",
        "            data,labels= \\\n",
        "                CustomDataLoader(test_data_CSV_path[batch_index:batch_index+batch_size],FeaturesNumber)\n",
        "            data,labels = data.to(device), labels.to(device)\n",
        "            'normalize'\n",
        "            data = (data - mean)/std\n",
        "            output = model(data)\n",
        "            #reshe for output\n",
        "            labels_reshaped = labels.to('cpu')\n",
        "            labels_reshaped = labels_reshaped.numpy()\n",
        "            labels_reshaped = labels_reshaped.reshape(-1,1)\n",
        "            output_reshaped = output.to('cpu')\n",
        "            output_reshaped = output_reshaped.numpy()\n",
        "            output_reshaped = output_reshaped.reshape(-1,1)\n",
        "            Labels_List_For_Whole_Column.append(labels_reshaped)\n",
        "            Output_List_For_Whole_Column.append(output_reshaped)\n",
        "    #save .csv concatenated\n",
        "    LabelsNumpy = np.concatenate(Labels_List_For_Whole_Column)\n",
        "    OutputNumpy = np.concatenate(Output_List_For_Whole_Column)\n",
        "    Output = np.hstack((LabelsNumpy,OutputNumpy))\n",
        "    Output = pd.DataFrame(Output, columns = ['LABELS', 'PREDICTIONS'])\n",
        "    if(zasebni_test_folder == True):\n",
        "      PathZaSejv = os.path.join(DATADIR,'SeparateDir/Results.csv')\n",
        "    else:\n",
        "      PathZaSejv = os.path.join(DATADIR,'Model/Results.csv')\n",
        "    Output.to_csv(PathZaSejv,columns=['LABELS','PREDICTIONS'])\n",
        "    print('Saved --> Results.csv with labels and predictions inside Model directory !!!')"
      ],
      "metadata": {
        "id": "E8p8mxGq5ZYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MAIN\n",
        "\n",
        "#in case of startaing anew\n",
        "if(os.path.isdir(os.path.join(DATADIR,'Model')) == True):\n",
        "        os.rmdir(os.path.join(DATADIR,'Model'))\n",
        "torch.cuda.empty_cache()\n",
        "#here i just stored precalculted mean std\n",
        "mean = torch.tensor([ 0.0011, -0.0023,  0.0020], dtype = torch.float32)\n",
        "std = torch.tensor([0.0242, 0.0273, 0.0212], dtype = torch.float32)\n",
        "#train\n",
        "TrainNetwork(FeaturesNumber = 3, LSTM_hidden_num = 300 ,Dropout_Value=0.1, epoch_num = 100,validation_patience=100,\n",
        "             batch_size = 5, learning_rate = .00001,\n",
        "             precalculated_mean_std = True, precalculated_mean = mean, precalculated_std = std)\n",
        "\n",
        "#test\n",
        "TestiranjeMreze(FeaturesNumber = 3, batch_size = 50,separate_test_dir = False,\n",
        "                precalculated_mean_std = False, precalculated_mean = mean, precalculated_std = std)"
      ],
      "metadata": {
        "id": "XwOPbtAOnppT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}